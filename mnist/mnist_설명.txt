mnist
일반 mnist는 단일 신경망
mnist_cnn은 다중 신경망이다.
고로 후자가 정확도가 약 0.07만큼 정확하나 시간이 더 걸린다.

오차 = 목표값 - 실제값
	- 오차란 실제값이 함수 그래프에서 얼마나 떨어져 있냐의 차이이다.
	- 함수 그래프란 모델.
	- 즉 GradientDescentOptimizer(0.01)는 오차률을 설정하여
	    얼마나 수용할 것인가를 뜻한다.
		- 여기서 범위를 0.1로 한다면 정확도가 바닥을 칠것이다.
		- 정보이득이다.
	- for i in range(1000):
	  batch_xs, batch_ys = mnist.train.next_batch(100)
	  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
	  위에서 러닝을 1000번 한다는 뜻이다. 하지만 반복횟수를 줄이는 순간
	  정확도 또한 바닥을 칠것이다. 하지만 1000번 이상이된다면
	  오히려 과적합으로 인하여 마찬가지로 정확도가 바닥을 칠 것이다.
	  실제로 10000하면 1000한것의 이하로 떨어진다.

6만개의 트레이닝 데이터와
1마내의 테스트 데이터를 사용한다.
			